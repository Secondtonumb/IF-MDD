# Hyperparameters toggles
prefix: ""

## SSL features Selection
pretrained_models_path: pretrained_models/
# pretrained_models:
# {
#     "wav2vec2_base": "facebook/wav2vec2-base", # 768
#     "hubert_base": "facebook/hubert-base-ls960", # 768
#     "wavlm_base": "microsoft/wavlm-base", # 768
#     "wavlm_base_plus": "microsoft/wavlm-base-plus", # 768
#     "hubert_multilingual": "utter-project/mHuBERT-147", # 768
#     "clap" : "laion/clap-htsat-fused", # 768
#     "data2vec_base": "facebook/data2vec-audio-base", # 768
    
#     "wav2vec2_large": "facebook/wav2vec2-large", # 1024
#     "hubert_large": "facebook/hubert-large-ls960", # 1024
#     "wavlm_large": "microsoft/wavlm-large-plus", # 1024
#     "data2vec_large": "facebook/data2vec-audio-large", #1024
#     "whisper_medium": "openai/whisper-medium", # 1024
    
#     "whisper_large_v3_turbo": "openai/whisper-large-v3-turbo", # 1280
# }


# select pretrained SSL models
perceived_ssl_model: "wav2vec2_base" # in pretrained_models
canonical_ssl_model: "wav2vec2_base" # in pretrained_models

# # models hidden size, varies by model
ENCODER_DIM: 768

# # How to fuse the features
feature_fusion: "dual_ssl_enc"        # Options: "mono" for single ssl, "dual_ssl_enc" for dual ssl encoder, "dual_loss" for single SSL dual ssl loss
blend_alpha: 0.2              # If using "blend" fusion

hybrid_lambda: 0.3              # If using hybrid attention, this is the alpha blending factor between CTC and attention outputs

# Input files
# Data files
data_folder_save: "./data"
train_annotation: !ref <data_folder_save>/train-train.json
valid_annotation: !ref <data_folder_save>/train-dev.json
test_annotation: !ref <data_folder_save>/test.json

# generate training id for output folder
# generate_training_id: !apply:trainer.generate_training_id.generate_training_id [!ref <perceived_ssl_model_id>, !ref <canonical_ssl_model_id>, !ref <feature_fusion>, !ref <prefix>]

# output files
output_folder: !ref exp_l2arctic/<perceived_ssl_model>_<canonical_ssl_model>_<feature_fusion>_<prefix>
per_file: !ref <output_folder>/per.txt
per_can_file: !ref <output_folder>/per_cano.txt
mpd_file: !ref <output_folder>/mpd.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

on_training_test_per_folder: !ref <output_folder>/on_training_test_per
on_training_test_mpd_folder: !ref <output_folder>/on_training_test_mpd

# Modules (SpeechBrain lobes)
modules:
    canonical_ssl: !ref <canonical_ssl>
    perceived_ssl: !ref <perceived_ssl>
    phn_emb: !ref <phn_emb>
    enc: !ref <enc>
    enc_can: !ref <enc_can>
    ctc_lin: !ref <ctc_lin>
    ctc_lin_can: !ref <ctc_lin_can>
    cross_attn: !ref <cross_attn>  
    attn_proj: !ref <attn_proj> 
    mispro_head: !ref <mispro_head>

perceived_ssl: !apply:trainer.AutoSSLoader.AutoSSLLoader
    model_name: !ref <perceived_ssl_model>
    freeze: !ref <freeze_perceived_ssl>
    freeze_feature_extractor: !ref <freeze_perceived_feature_extractor>
    save_path: !ref <pretrained_models_path>
    output_all_hiddens: False
preceived_ssl_emb_layer: -1

canonical_ssl: !apply:trainer.AutoSSLoader.AutoSSLLoader
    model_name: !ref <canonical_ssl_model>
    freeze: !ref <freeze_canonical_ssl>
    freeze_feature_extractor: !ref <freeze_perceived_feature_extractor>
    save_path: !ref <pretrained_models_path>
    output_all_hiddens: False
canonical_ssl_emb_layer: -1

# Text prompt module
phn_emb: !new:torch.nn.Sequential
    - !new:torch.nn.Embedding
      num_embeddings: !ref <output_neurons>
      embedding_dim: !ref <dnn_neurons>
    - !new:torch.nn.LayerNorm
      normalized_shape: !ref <dnn_neurons>

enc: !new:torch.nn.Sequential
  - !new:speechbrain.lobes.models.VanillaNN.VanillaNN
     input_shape: [null, null, !ref <ENCODER_DIM>]
     activation: !ref <activation>
     dnn_blocks: !ref <dnn_layers>
     dnn_neurons: !ref <dnn_neurons>
  - !new:torch.nn.LayerNorm
     normalized_shape: !ref <dnn_neurons>
   

enc_can: !new:torch.nn.Sequential
  - !new:speechbrain.lobes.models.VanillaNN.VanillaNN
     input_shape: [null, null, !ref <ENCODER_DIM>]
     activation: !ref <activation>
     dnn_blocks: !ref <dnn_layers>
     dnn_neurons: !ref <dnn_neurons>
  - !new:torch.nn.LayerNorm
     normalized_shape: !ref <dnn_neurons>

ctc_lin:  !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dnn_neurons>
    n_neurons: !ref <output_neurons>  # 40 phonemes + 1 blank + 1 err

ctc_lin_can:  !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dnn_neurons>
    n_neurons: !ref <output_neurons>  # 40 phonemes + 1 blank + 1 err

cross_attn: !new:speechbrain.nnet.attention.MultiheadAttention
    nhead: 8  # Number of attention heads, adjust as needed
    d_model: !ref <dnn_neurons>
    dropout: 0.1  # Dropout rate for attention weights

attn_proj: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dnn_neurons>
    n_neurons: !ref <output_neurons>  # Projected dimension after attention, adjust as needed

mispro_head: !new:torch.nn.Linear
    in_features: !ref <dnn_neurons>
    out_features: 4   # 4 classes: 0(no mispro), 1(sub), 2(del), 3(ins)

# Model parameters
activation: !name:torch.nn.LeakyReLU
dnn_layers: 2
dnn_neurons: 384  
freeze_perceived_ssl: False
freeze_canonical_ssl: False
freeze_perceived_feature_extractor: True  # freeze the CNN extractor in wav2vec
freeze_canonical_feature_extractor: True         # Freeze Whisper encoder?

log_softmax: !new:speechbrain.nnet.activations.Softmax
    apply_log: True

ctc_cost: !name:speechbrain.nnet.losses.ctc_loss
    blank_index: !ref <blank_index>
    
# mispro_cost: !name:speechbrain.nnet.losses.bce_loss
#     reduction: batchmean
#     pos_weight: !apply:torch.Tensor.to
#         - !new:torch.Tensor
#             - [2.0]
#         - "cuda:0"

mispro_cost: !name:speechbrain.nnet.losses.kldiv_loss
    reduction: batchmean

# Outputs
output_neurons: 42 # l2arctic: 40phns(sil)+err+blank=42 # for erj 41 as no err in ERJ
blank_index: 0

model: !new:torch.nn.ModuleList
    - [!ref <enc>, !ref <ctc_lin>, !ref <cross_attn>, !ref <attn_proj>, !ref <mispro_head>]

model_cano: !new:torch.nn.ModuleList
    - [!ref <enc_can>, !ref <ctc_lin_can>]

adam_opt_class: !name:torch.optim.Adam
    lr: !ref <lr>

adam_opt_class_cano: !name:torch.optim.Adam
    lr: !ref <lr>

pretrained_opt_class: !name:torch.optim.Adam
    lr: !ref <lr_pretrained>

pretrained_opt_cano_class: !name:torch.optim.Adam
    lr: !ref <lr_pretrained>

mispronunciation_weight: 0

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        model_cano: !ref <model_cano>
        perceived_ssl: !ref <perceived_ssl>
        canonical_ssl: !ref <canonical_ssl>
        counter: !ref <epoch_counter>
    
# canonical_ssl: !ref <canonical_ssl>

augmentation: !new:speechbrain.augment.time_domain.SpeedPerturb
    orig_freq: !ref <sample_rate>
    speeds: [95, 100, 105]

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

ctc_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.ctc_loss
        blank_index: !ref <blank_index>
        reduction: batch

ctc_stats_can: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.ctc_loss
        blank_index: !ref <blank_index>
        reduction: batch

per_stats: !name:speechbrain.utils.metric_stats.ErrorRateStats

per_stats_can: !name:speechbrain.utils.metric_stats.ErrorRateStats

mispro_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.kldiv_loss
        reduction: batch

# # TIMIT
# timit_local_data_folder: "/common/db/TIMIT"  # Path to TIMIT datase

seed: 3047
__set_seed: !apply:torch.manual_seed [!ref <seed>]

# training parameters
number_of_epochs: 100
batch_size: 16
lr: 0.0003
sorting: ascending
sample_rate: 16000
gradient_accumulation: 2
lr_pretrained: 0.00001

# Mix-Precision Training
auto_mix_prec: true
# or
precision: fp16         # 支持 "fp32"、"fp16" 或 "bf16"
eval_precision: fp16    # 推理同样切换到 FP16

# Dataloader options
train_dataloader_opts:
    batch_size: !ref <batch_size>
    

valid_dataloader_opts:
    batch_size: !ref <batch_size>
    

test_dataloader_opts:
    batch_size: 1
    
